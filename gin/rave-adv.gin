# -*-Python-*-
# Decodes from (loudness, f0). Has a trainable reverb component as well.
# Uses a RAVE-style decoder based on CNNs, decodes directly to the waveform
# (doesn't use harmonics+noise) like DDSP

include 'models/ae.gin'
import thesis.rave
import thesis.pqmf
import thesis.adversarial

get_model.model = @AdversarialVAE()

#AdversarialModel.base_model = @models.Autoencoder()

sample_rate = 16000
# in fullrave, n_samples must be a multiple of product(ratios) = 256
n_samples = 64000

# Constant values here: https://github.com/caillonantoine/RAVE/blob/1.0/train_rave.py
n_bands = 16  # because DATA_SIZE = 16
# LATENT_SIZE = 128 (the input to the network)
# CAPACITY = 64 (the output of the network. Each subsampling layer doubles the #samples and halves the hidden size,
# reaching the hidden size CAPACITY in the end.
# So there is no one equivalent value of `ch`.
decoder_output_channels = 128


AdversarialVAE:
    preprocessor = @PQMFAnalysis()
    encoder = @RAVECNNEncoder()
    decoder = @decoders.DilatedConvDecoder()
    kl_loss_weight = 0.1  # RAVE seems to have used this value before using a cyclic schedule


RAVECNNEncoder:
    input_keys = ("audio_multiband", )
    capacity = 64
    latent_size = 128
    # TODO: try the original ratios of [4, 4, 4, 2]?
    #       Now we use DilatedConvDecoder which requires a fixed ratio (stride) so it's not possible.
    ratios = [4, 4, 4, 4]
    bias = True


DilatedConvDecoder:
    ch = %decoder_output_channels
    layers_per_stack = 3  # Hardcoded in RAVE
    kernel_size = 3  # Hardcoded in RAVE
#    norm_type = 'batch' # TODO: is RAVE using batch norm? Do we need it?
    norm_type = "layer"
    input_keys = ("z", )
    # In RAVE, RATIOS = [4, 4, 4, 2] meaning there's 4 stacks, upsampling with a ratio of 4x4x4x2=128
    stacks = 4
    resample_stride = 4

    conditioning_keys = None  # Nothing else than a latent, so no need to consider this separately
    precondition_stack = None  # Not relevant since `conditioning_keys = None`

    output_splits = (('control_embedding', %decoder_output_channels),)
    # RAVE uses this (resamples *before* convolution), though it has one extra layer before the first resampling
    resample_after_convolve = False

# ---------------------
#https://github.com/ben-hayes/neural-waveshaping-synthesis/blob/main/neural_waveshaping_synthesis/models/neural_waveshaping.py#L30

# ==============
# ProcessorGroup
# ==============

#ProcessorGroup.dag = [
#  (@processors.Crop(),
#    ['control_embedding']),
#  (@RAVEWaveformGenerator(),
#    ['crop/signal']),
#  (@RAVEDownsamplingCNN(),
#    ['control_embedding']),
#  (@MultibandFilteredNoise(),
#    ['rave_downsampling_cnn/signal']),
#  (@processors.Add(),
#    ['multiband_filtered_noise/signal', 'rave_waveform_generator/signal']),
#  (@PQMFSynthesis(),
#    ['add/signal']),
##  (@effects.Reverb(),
##    ['pqmf_synthesis/signal']),
#]

# No noise generator.
ProcessorGroup.dag = [
  (@processors.Crop(),
    ['control_embedding']),
  (@RAVEWaveformGenerator(),
    ['crop/signal']),
  (@PQMFSynthesis(),
    ['rave_waveform_generator/signal']),
]

# Reverb
Reverb:
    name = 'reverb'
    reverb_length = 48000
    trainable = True

PQMFBank:
    attenuation = 100
    n_bands = %n_bands

PQMFAnalysis:
    pqmf_bank = @PQMFBank()

PQMFSynthesis:
    pqmf_bank = @PQMFBank()

RAVEWaveformGenerator:
    n_bands = %n_bands
    n_samples = %n_samples

RAVEDownsamplingCNN:
    ch = 128
    n_layers = 3
    downsample_per_layer = 4
    n_bands = %n_bands
    n_noise_bands = 5

MultibandFilteredNoise:
    n_bands = %n_bands
    n_samples = %n_samples

# We produce 65536 samples and need 64000, so cut off this many
Crop:
    frame_size = 96
    crop_location = 'back'

# Only relevant with the noise generator
# train_util.train.model_specific_summary_fn = @summarize_rave
train_util.train.model_specific_summary_fn = @thesis.util.summarize_generic

get_trainer_class.trainer_class = @thesis.adversarial.AdversarialTrainer

thesis.adversarial.AdversarialTrainer:
    learning_rate = %learning_rate
    lr_decay_steps = %lr_decay_steps
    lr_decay_rate = %lr_decay_rate
    grad_clip_norm = %grad_clip_norm
    checkpoints_to_keep = %checkpoints_to_keep
    warmup_steps = 250000
