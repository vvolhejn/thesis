{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import yaml\n",
    "import tqdm.notebook as tqdm\n",
    "import soundfile\n",
    "import numpy as np\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "import gin\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "from thesis.notebook_util import audio_bytes_to_np, play_audio, specplot\n",
    "from thesis.timbre_transfer_util import get_lufs, normalize_lufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\"dataset\": \"Violin\", \"artifact_name\": \"0725-ddspae-2\", \"display_name\": \"DDSP-full\"},\n",
    "    {\"dataset\": \"Violin\", \"artifact_name\": \"0725-ddspae-cnn-1-rt\", \"display_name\": \"DDSP-CNN\"},\n",
    "    {\"dataset\": \"Violin\", \"artifact_name\": \"0809-ddspae-cnn-5-rt\", \"display_name\": \"DDSP-CNN-Tiny\"},\n",
    "    {\"dataset\": \"Violin\", \"artifact_name\": \"0725-ddspae-cnn-1-rtq\", \"display_name\": \"DDSP-CNN\"},\n",
    "    {\"dataset\": \"Violin\", \"artifact_name\": \"0809-ddspae-cnn-5-rtq\", \"display_name\": \"DDSP-CNN-Tiny\"},\n",
    "# ]\n",
    "\n",
    "# models_trumpet = [\n",
    "    {\"dataset\": \"Trumpet\", \"artifact_name\": \"0805-ddspae\", \"display_name\": \"DDSP-full\"},\n",
    "    {\"dataset\": \"Trumpet\", \"artifact_name\": \"0804-ddspae-cnn-3-rt\", \"display_name\": \"DDSP-CNN\"},\n",
    "    {\"dataset\": \"Trumpet\", \"artifact_name\": \"0809-ddspae-cnn-4-rt\", \"display_name\": \"DDSP-CNN-Tiny\"},\n",
    "    {\"dataset\": \"Trumpet\", \"artifact_name\": \"0804-ddspae-cnn-3-rtq\", \"display_name\": \"DDSP-CNN\"},\n",
    "    {\"dataset\": \"Trumpet\", \"artifact_name\": \"0809-ddspae-cnn-4-rtq\", \"display_name\": \"DDSP-CNN-Tiny\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "def load_models(models):\n",
    "    for model in models:\n",
    "        artifact = api.artifact(\n",
    "            f\"neural-audio-synthesis-thesis/nas-evaluation/eval-{model['artifact_name']}:latest\"\n",
    "        )\n",
    "        model[\"run\"] = artifact.logged_by()\n",
    "\n",
    "        print(artifact.created_at, model[\"artifact_name\"])\n",
    "\n",
    "load_models(models)\n",
    "# load_models(models_trumpet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLES_DIR = \"/Users/vaclav/prog/thesis/data/wandb-samples\"\n",
    "\n",
    "\n",
    "def download_audio_samples(run, samples_dir=SAMPLES_DIR, name_must_contain=\"audio_both\"):\n",
    "    paths = []\n",
    "    for file in tqdm.tqdm(run.files(), leave=False):\n",
    "        if file.mimetype.startswith(\"audio\"):\n",
    "            if name_must_contain in file.name:\n",
    "                path = os.path.join(SAMPLES_DIR, file.name)\n",
    "\n",
    "                if os.path.isfile(path):\n",
    "                    paths.append(path)\n",
    "                else:\n",
    "                    f = file.download(samples_dir, replace=False)\n",
    "                    paths.append(f.name)\n",
    "                    f.close()\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for model in tqdm.tqdm(models):\n",
    "    model[\"paths\"] = download_audio_samples(model[\"run\"])\n",
    "    model[\"tt_paths\"] = download_audio_samples(model[\"run\"], name_must_contain=\"timbre_transfer\")\n",
    "    \n",
    "# paths1 = download_audio_samples(run1)\n",
    "# paths2 = download_audio_samples(run1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webmushra_dir = \"/Users/vaclav/prog/webMUSHRA/\"\n",
    "\n",
    "max_clips = 15\n",
    "\n",
    "def prepare_samples(models, subdir, paths_key=\"paths\", included_indices=None):\n",
    "    for model in models:\n",
    "        assert len(model[paths_key]) == len(models[0][paths_key]), f\"Different number of samples in {model['artifact_name']} {model['run'].id}\"\n",
    "\n",
    "#     out_dir = os.path.join(SAMPLES_DIR, \"preprocessed\", subdir)\n",
    "    out_dir = os.path.join(webmushra_dir, \"audio\", subdir)\n",
    "    \n",
    "    if os.path.isdir(out_dir):\n",
    "        for f in os.listdir(out_dir):\n",
    "            if f.startswith(\"sample\") and f.endswith(\".wav\"):\n",
    "                os.remove(os.path.join(out_dir, f))\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    processed_paths = []\n",
    "    \n",
    "    for j, cur_paths in tqdm.tqdm(list(enumerate(zip(*[p[paths_key] for p in models])))):\n",
    "        if included_indices is not None and j not in included_indices:\n",
    "            continue\n",
    "        \n",
    "        # audios = [[] for _ in range(len(paths_per_model) + 1)]\n",
    "        audio_length = None\n",
    "        audio_gt = None\n",
    "        lufs = None\n",
    "        skip = False\n",
    "        cur_processed_paths = []\n",
    "        \n",
    "        for i, path in enumerate(cur_paths):\n",
    "            with open(path, \"rb\") as f:\n",
    "                audio_cur = audio_bytes_to_np(f.read(), normalize_db=None)\n",
    "\n",
    "                if not audio_length:\n",
    "                    audio_length = len(audio_cur)\n",
    "\n",
    "                audio_pred_cur = audio_cur[:audio_length // 2]\n",
    "                audio_gt_cur = audio_cur[audio_length // 2:]\n",
    "\n",
    "                if audio_gt is None:\n",
    "                    audio_gt = audio_gt_cur\n",
    "                    # audios[-1].append(audio_gt)\n",
    "                    lufs = get_lufs(audio_gt)\n",
    "                    if lufs < -40:\n",
    "                        print(f\"Loudness is {lufs:.2f} LUFS for sample {j}, skipping\")\n",
    "                        skip = True\n",
    "                    else:\n",
    "                        audio_gt_cur = normalize_lufs(audio_gt_cur, target_lufs=-20, actual_lufs=lufs)\n",
    "                        assert np.max(np.abs(audio_gt_cur)) <= 1.0\n",
    "                        output_path = os.path.join(out_dir, f\"sample_{j}_gt.wav\")\n",
    "                        soundfile.write(output_path, audio_gt_cur, samplerate=16000)\n",
    "                        cur_processed_paths.append(output_path)\n",
    "                else:\n",
    "                    assert np.allclose(audio_gt_cur, audio_gt)\n",
    "\n",
    "            if skip:\n",
    "                break\n",
    "            \n",
    "            audio_pred_cur = normalize_lufs(audio_pred_cur, target_lufs=-20)\n",
    "            output_path = os.path.join(out_dir, f\"sample_{j}_{models[i]['artifact_name']}.wav\")\n",
    "            soundfile.write(output_path, audio_pred_cur, samplerate=16000)\n",
    "            cur_processed_paths.append(output_path)\n",
    "        \n",
    "        if not skip:\n",
    "            processed_paths.append(cur_processed_paths)\n",
    "            if len(processed_paths) == max_clips:\n",
    "                break\n",
    "    \n",
    "    return processed_paths\n",
    "\n",
    "violin_processed_paths = prepare_samples(models[:5], subdir=\"violin\")\n",
    "trumpet_processed_paths = prepare_samples(models[5:], subdir=\"trumpet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_indices = [\n",
    "    2,  # guitar (for training)\n",
    "    8,  # clarinet\n",
    "#     10,  # voice (m)\n",
    "    11,  # flute\n",
    "    13,  # voice (m)\n",
    "    20,  # sax\n",
    "    23,  # guitar\n",
    "    29,  # voice (f)\n",
    "]\n",
    "violin_tt_processed_paths = prepare_samples(\n",
    "    models[:5], subdir=\"violin_tt\", paths_key=\"tt_paths\", included_indices=tt_indices,\n",
    ")\n",
    "trumpet_tt_processed_paths = prepare_samples(\n",
    "    models[5:], subdir=\"trumpet_tt\", paths_key=\"tt_paths\", included_indices=tt_indices,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- type: mushra\n",
    "            id: trial_random_1\n",
    "            name: MUSHRA - Random 2\n",
    "            content: something something\n",
    "            enableLooping: true \n",
    "            reference: configs/resources/audio/mono_ref.wav\n",
    "            createAnchor35: false\n",
    "            createAnchor70: false\n",
    "            stimuli:\n",
    "                C1: configs/resources/audio/mono_c1.wav\n",
    "                C2: configs/resources/audio/mono_c2.wav\n",
    "                C3: configs/resources/audio/mono_c3.wav          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_config_template = {\n",
    "    \"type\": \"mushra\",\n",
    "    \"id\": None,\n",
    "    \"content\": \"Please rate the naturalness of all conditions by comparing them to the reference.\",\n",
    "    \"enableLooping\": True,\n",
    "    \"reference\": None,\n",
    "    \"createAnchor35\": True,\n",
    "    \"createAnchor70\": False,  # Limited by sample rate anyways\n",
    "    \"stimuli\": [],\n",
    "}\n",
    "\n",
    "def create_page_config(processed_paths, models, overrides=None):\n",
    "    if overrides is None:\n",
    "        overrides = {}\n",
    "\n",
    "    stimuli_config = [\"random\"]\n",
    "\n",
    "    for sample_paths in processed_paths:\n",
    "        cur_yaml = page_config_template.copy()\n",
    "        cur_yaml[\"id\"] = (\n",
    "            os.path.basename(os.path.dirname(sample_paths[0]))\n",
    "            + \"_\"\n",
    "            + os.path.basename(sample_paths[0])\n",
    "        )\n",
    "#         cur_yaml[\"content\"] = \"Neural audio synthesis sample\"\n",
    "        cur_yaml[\"reference\"] = os.path.relpath(sample_paths[0], webmushra_dir)\n",
    "    \n",
    "        for k, v in overrides.items():\n",
    "            cur_yaml[k] = v\n",
    "\n",
    "        cur_yaml[\"stimuli\"] = {}\n",
    "        for i in range(len(models)):\n",
    "            cur_yaml[\"stimuli\"][models[i][\"artifact_name\"]] = os.path.relpath(sample_paths[i + 1], webmushra_dir)\n",
    "\n",
    "        stimuli_config.append(cur_yaml)\n",
    "    \n",
    "    return stimuli_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_generated_yaml():\n",
    "    webmushra_config_dir = os.path.join(webmushra_dir, \"configs\")\n",
    "    template_filename = \"thesis_template.yaml\"\n",
    "    with open(os.path.join(webmushra_config_dir, template_filename), \"r\") as f:\n",
    "        webmushra_config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "    n_samples = 3\n",
    "    webmushra_config[\"pages\"][webmushra_config[\"pages\"].index(\"DUMMY_VIOLIN\")] = create_page_config(\n",
    "        violin_processed_paths[:n_samples*2:2], models[:5]\n",
    "    )\n",
    "    webmushra_config[\"pages\"][webmushra_config[\"pages\"].index(\"DUMMY_TRUMPET\")] = create_page_config(\n",
    "        trumpet_processed_paths[:n_samples*2:2], models[5:]\n",
    "    )\n",
    "\n",
    "    tt_description_template = (\n",
    "        \"Please rate the naturalness of all conditions. \"\n",
    "        \"<b>The conditions are supposed sound like {}.</b> \"\n",
    "        \"<br>Due to technical limitations, the reference sound is also included among the conditions. You may leave it ungraded.\"\n",
    "    )\n",
    "    \n",
    "    webmushra_config[\"pages\"][webmushra_config[\"pages\"].index(\"DUMMY_VIOLIN_TT\")] = create_page_config(\n",
    "        violin_tt_processed_paths, models[:5],\n",
    "        overrides={\n",
    "            \"createAnchor35\": False,\n",
    "            \"content\": tt_description_template.format(\"a violin\")\n",
    "        },\n",
    "    )\n",
    "#     webmushra_config[\"pages\"][webmushra_config[\"pages\"].index(\"DUMMY_TRUMPET_TT\")] = create_page_config(\n",
    "#         trumpet_tt_processed_paths, models[5:],\n",
    "#         overrides={\n",
    "#             \"createAnchor35\": False,\n",
    "#             \"content\": tt_description_template.format(\"a trumpet\")\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "    data_str = yaml.safe_dump(webmushra_config)\n",
    "\n",
    "    # To deal with parsing bug in webmushra\n",
    "    data_str = data_str.replace(\"\\n- - random\\n\", \"\\n-\\n  - random\\n\")\n",
    "\n",
    "    with open(os.path.join(webmushra_config_dir, \"thesis_generated.yaml\"), \"w\") as f:\n",
    "        f.write(data_str)\n",
    "\n",
    "update_generated_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    update_generated_yaml()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
